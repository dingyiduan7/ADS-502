{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = pd.read_csv(\"/Users/antran/Google Drive/USD_Teaching/ADS-502/WebsiteDataSets/bank-additional.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for simplicity and demonstration purposes, only a few predictors along with the target variable are selected "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank = bank[['job', 'marital', 'housing', 'loan', 'y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To partition the data set, we will use the command `train_test_split()`. Let's use 25% of our data as our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_train, bank_test = train_test_split(bank, \n",
    "                                         test_size = 0.25,\n",
    "                                         random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the data set was partitioned correctly, you can compare the\n",
    "shapes of the original, training, and test data sets using the shape feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of instances before partitioning:  4119 \n",
      "Number of instances in Training set:  3089 \n",
      "Number of instances in Test set:  1030\n"
     ]
    }
   ],
   "source": [
    "print('Original number of instances before partitioning: ', bank.shape[0],\n",
    "      '\\nNumber of instances in Training set: ', bank_train.shape[0],\n",
    "      '\\nNumber of instances in Test set: ', bank_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of training instances:  74.99393056567128 \n",
      "Proportion of test instances:  25.00606943432872\n"
     ]
    }
   ],
   "source": [
    "print('Proportion of training instances: ', bank_train.shape[0]/bank.shape[0]*100,\n",
    "      '\\nProportion of test instances: ', bank_test.shape[0]/bank.shape[0]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balance the Training Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we identify how many records in `bank_train` have the less common value,\n",
    "`yes` for response, using the `value_counts()` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     2751\n",
       "yes     338\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_train['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.942052444156685"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = bank_train['y'].value_counts()[1] / bank_train.shape[0] * 100\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The count of `yes` responses will change depending on the partition. Say, we want to increase the percentage of `yes` responses to 30%. Since we have p = 0.3, we need to resample ~800 records whose response is `yes` and add\n",
    "them to our training data set. To begin resampling, we isolate the records which we want to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_resample = bank_train.loc[bank_train['y'] == \"yes\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to sample from our records of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_resample = to_resample.sample(n = 840, replace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add the resampled records to our original training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_train_rebal = pd.concat([bank_train, our_resample], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the desired percent of `yes` responses was obtained, examine\n",
    "the table of the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no     2751\n",
       "yes    1178\n",
       "Name: y, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bank_train_rebal['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29.98218376177144"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = bank_train_rebal['y'].value_counts()[1] / bank_train_rebal.shape[0] * 100\n",
    "ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, a good practice is to split predictors and target variables into different objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = bank_train_rebal[['job', 'marital', 'housing', 'loan']]\n",
    "y_train = bank_train_rebal['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, sklearn does not automatically handle categorical variables. This means we need to convert our categorical variables into dummy variables versions of themselves before we can run the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = pd.DataFrame()\n",
    "\n",
    "for var in X_train.columns:\n",
    "    dummies = pd.get_dummies(X_train[var])\n",
    "    X_train_processed = pd.concat([X_train_processed, dummies], axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we run the Naïve Bayes algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB().fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions on the test data using the Naïve Bayes model, we first need to set up the X variables in the test data set the exact same way as we did for the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = bank_test[['job', 'marital', 'housing', 'loan']]\n",
    "y_test = bank_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_processed = pd.DataFrame()\n",
    "\n",
    "for var in X_test.columns:\n",
    "    dummies = pd.get_dummies(X_test[var])\n",
    "    X_test_processed = pd.concat([X_test_processed, dummies], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nb.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[899,  18],\n",
       "       [109,   4]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TN:  899 \n",
      "FP:  18 \n",
      "FN:  109 \n",
      "TP:  4\n"
     ]
    }
   ],
   "source": [
    "TN = cm[0][0]\n",
    "FP = cm[0][1]\n",
    "FN = cm[1][0]\n",
    "TP = cm[1][1]\n",
    "\n",
    "print('TN: ', TN,\n",
    "      '\\nFP: ', FP,\n",
    "      '\\nFN: ', FN,\n",
    "      '\\nTP: ', TP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Accuracy = \\frac{TN+TP}{TN+TP+FP+FN} $$\n",
    "\n",
    "$$ Error Rate = 1 - Accuracy $$\n",
    "\n",
    "$$ Sensitivity = Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "$$ Specificity = \\frac{TN}{TN+FP} $$\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP} $$\n",
    "\n",
    "$$ F1 = 2* \\frac{Precision*Recall}{Precision + Recall} $$\n",
    "\n",
    "$$ F2 = 5* \\frac{Precision*Recall}{4*Precision + Recall} $$\n",
    "\n",
    "$$ F0.5 = 1.25* \\frac{Precision*Recall}{0.25*Precision + Recall} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
